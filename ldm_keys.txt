alphas_cumprod: torch.Size([1000])
alphas_cumprod_prev: torch.Size([1000])
betas: torch.Size([1000])
cond_stage_model.embeddings.LayerNorm.bias: torch.Size([768])
cond_stage_model.embeddings.LayerNorm.weight: torch.Size([768])
cond_stage_model.embeddings.position_embeddings.weight: torch.Size([514, 768])
cond_stage_model.embeddings.token_type_embeddings.weight: torch.Size([1, 768])
cond_stage_model.embeddings.word_embeddings.weight: torch.Size([50265, 768])
cond_stage_model.encoder.layer.0.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.0.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.0.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.0.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.0.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.0.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.0.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.0.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.0.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.0.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.0.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.0.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.0.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.0.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.0.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.0.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.1.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.1.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.1.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.1.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.1.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.1.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.1.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.1.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.1.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.1.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.1.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.1.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.1.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.1.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.1.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.1.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.10.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.10.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.10.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.10.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.10.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.10.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.10.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.10.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.10.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.10.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.10.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.10.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.10.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.10.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.10.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.10.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.11.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.11.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.11.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.11.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.11.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.11.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.11.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.11.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.11.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.11.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.11.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.11.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.11.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.11.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.11.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.11.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.2.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.2.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.2.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.2.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.2.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.2.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.2.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.2.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.2.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.2.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.2.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.2.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.2.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.2.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.2.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.2.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.3.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.3.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.3.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.3.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.3.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.3.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.3.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.3.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.3.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.3.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.3.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.3.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.3.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.3.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.3.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.3.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.4.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.4.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.4.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.4.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.4.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.4.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.4.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.4.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.4.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.4.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.4.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.4.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.4.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.4.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.4.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.4.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.5.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.5.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.5.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.5.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.5.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.5.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.5.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.5.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.5.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.5.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.5.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.5.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.5.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.5.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.5.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.5.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.6.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.6.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.6.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.6.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.6.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.6.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.6.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.6.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.6.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.6.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.6.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.6.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.6.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.6.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.6.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.6.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.7.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.7.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.7.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.7.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.7.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.7.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.7.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.7.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.7.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.7.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.7.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.7.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.7.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.7.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.7.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.7.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.8.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.8.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.8.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.8.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.8.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.8.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.8.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.8.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.8.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.8.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.8.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.8.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.8.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.8.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.8.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.8.output.dense.weight: torch.Size([768, 3072])
cond_stage_model.encoder.layer.9.attention.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.9.attention.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.9.attention.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.9.attention.output.dense.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.9.attention.self.key.bias: torch.Size([768])
cond_stage_model.encoder.layer.9.attention.self.key.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.9.attention.self.query.bias: torch.Size([768])
cond_stage_model.encoder.layer.9.attention.self.query.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.9.attention.self.value.bias: torch.Size([768])
cond_stage_model.encoder.layer.9.attention.self.value.weight: torch.Size([768, 768])
cond_stage_model.encoder.layer.9.intermediate.dense.bias: torch.Size([3072])
cond_stage_model.encoder.layer.9.intermediate.dense.weight: torch.Size([3072, 768])
cond_stage_model.encoder.layer.9.output.LayerNorm.bias: torch.Size([768])
cond_stage_model.encoder.layer.9.output.LayerNorm.weight: torch.Size([768])
cond_stage_model.encoder.layer.9.output.dense.bias: torch.Size([768])
cond_stage_model.encoder.layer.9.output.dense.weight: torch.Size([768, 3072])
first_stage_model.decoder.cnn_decoder.conv_in.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.conv_in.weight: torch.Size([256, 16, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.conv_out.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.conv_out.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.mid.attn_1.k.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.attn_1.k.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.mid.attn_1.norm.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.attn_1.norm.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.attn_1.proj_out.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.attn_1.proj_out.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.mid.attn_1.q.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.attn_1.q.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.mid.attn_1.v.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.attn_1.v.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.mid.block_1.conv1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_1.conv1.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.mid.block_1.conv2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_1.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.mid.block_1.norm1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_1.norm1.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_1.norm2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_1.norm2.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_2.conv1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_2.conv1.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.mid.block_2.conv2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_2.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.mid.block_2.norm1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_2.norm1.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_2.norm2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.mid.block_2.norm2.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.norm_out.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.norm_out.weight: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.0.conv1.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.0.conv1.weight: torch.Size([64, 128, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.0.block.0.conv2.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.0.conv2.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.0.block.0.nin_shortcut.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.0.nin_shortcut.weight: torch.Size([64, 128, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.0.block.0.norm1.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.0.block.0.norm1.weight: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.0.block.0.norm2.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.0.norm2.weight: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.1.conv1.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.1.conv1.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.0.block.1.conv2.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.1.conv2.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.0.block.1.norm1.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.1.norm1.weight: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.1.norm2.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.1.norm2.weight: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.2.conv1.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.2.conv1.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.0.block.2.conv2.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.2.conv2.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.0.block.2.norm1.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.2.norm1.weight: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.2.norm2.bias: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.0.block.2.norm2.weight: torch.Size([64])
first_stage_model.decoder.cnn_decoder.up.1.block.0.conv1.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.0.conv1.weight: torch.Size([128, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.1.block.0.conv2.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.0.conv2.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.1.block.0.nin_shortcut.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.0.nin_shortcut.weight: torch.Size([128, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.1.block.0.norm1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.1.block.0.norm1.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.1.block.0.norm2.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.0.norm2.weight: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.1.conv1.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.1.conv1.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.1.block.1.conv2.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.1.conv2.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.1.block.1.norm1.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.1.norm1.weight: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.1.norm2.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.1.norm2.weight: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.2.conv1.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.2.conv1.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.1.block.2.conv2.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.2.conv2.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.1.block.2.norm1.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.2.norm1.weight: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.2.norm2.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.block.2.norm2.weight: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.upsample.conv.bias: torch.Size([128])
first_stage_model.decoder.cnn_decoder.up.1.upsample.conv.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.k.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.k.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.norm.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.norm.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.proj_out.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.proj_out.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.q.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.q.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.v.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.0.v.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.k.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.k.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.norm.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.norm.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.proj_out.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.proj_out.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.q.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.q.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.v.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.1.v.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.k.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.k.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.norm.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.norm.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.proj_out.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.proj_out.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.q.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.q.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.v.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.attn.2.v.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.decoder.cnn_decoder.up.2.block.0.conv1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.0.conv1.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.2.block.0.conv2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.0.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.2.block.0.norm1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.0.norm1.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.0.norm2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.0.norm2.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.1.conv1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.1.conv1.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.2.block.1.conv2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.1.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.2.block.1.norm1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.1.norm1.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.1.norm2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.1.norm2.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.2.conv1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.2.conv1.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.2.block.2.conv2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.2.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.cnn_decoder.up.2.block.2.norm1.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.2.norm1.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.2.norm2.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.block.2.norm2.weight: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.upsample.conv.bias: torch.Size([256])
first_stage_model.decoder.cnn_decoder.up.2.upsample.conv.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.0.bias: torch.Size([64])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.0.weight: torch.Size([64, 32])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.1.bias: torch.Size([64])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.1.weight: torch.Size([64, 64])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.2.bias: torch.Size([64])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.2.weight: torch.Size([64, 64])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.3.bias: torch.Size([64])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.3.weight: torch.Size([64, 64])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.4.bias: torch.Size([64])
first_stage_model.decoder.gino_decoder.gno_out.mlp.fcs.4.weight: torch.Size([64, 64])
first_stage_model.decoder.gino_decoder.pos_embed.linear.bias: torch.Size([8])
first_stage_model.decoder.gino_decoder.pos_embed.linear.weight: torch.Size([8, 3])
first_stage_model.decoder.gino_decoder.projection.fcs.0.bias: torch.Size([3])
first_stage_model.decoder.gino_decoder.projection.fcs.0.weight: torch.Size([3, 64])
first_stage_model.encoder.cnn_encoder.conv_in.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.conv_in.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.conv_out.bias: torch.Size([32])
first_stage_model.encoder.cnn_encoder.conv_out.weight: torch.Size([32, 256, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.0.block.0.conv1.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.0.conv1.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.0.block.0.conv2.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.0.conv2.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.0.block.0.norm1.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.0.norm1.weight: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.0.norm2.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.0.norm2.weight: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.1.conv1.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.1.conv1.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.0.block.1.conv2.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.1.conv2.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.0.block.1.norm1.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.1.norm1.weight: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.1.norm2.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.block.1.norm2.weight: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.downsample.conv.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.0.downsample.conv.weight: torch.Size([64, 64, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.1.block.0.conv1.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.0.conv1.weight: torch.Size([128, 64, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.1.block.0.conv2.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.0.conv2.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.1.block.0.nin_shortcut.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.0.nin_shortcut.weight: torch.Size([128, 64, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.1.block.0.norm1.bias: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.1.block.0.norm1.weight: torch.Size([64])
first_stage_model.encoder.cnn_encoder.down.1.block.0.norm2.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.0.norm2.weight: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.1.conv1.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.1.conv1.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.1.block.1.conv2.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.1.conv2.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.1.block.1.norm1.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.1.norm1.weight: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.1.norm2.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.block.1.norm2.weight: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.downsample.conv.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.1.downsample.conv.weight: torch.Size([128, 128, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.k.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.k.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.norm.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.norm.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.proj_out.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.proj_out.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.q.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.q.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.v.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.0.v.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.k.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.k.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.norm.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.norm.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.proj_out.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.proj_out.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.q.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.q.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.v.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.attn.1.v.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.block.0.conv1.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.0.conv1.weight: torch.Size([256, 128, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.2.block.0.conv2.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.0.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.2.block.0.nin_shortcut.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.0.nin_shortcut.weight: torch.Size([256, 128, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.down.2.block.0.norm1.bias: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.2.block.0.norm1.weight: torch.Size([128])
first_stage_model.encoder.cnn_encoder.down.2.block.0.norm2.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.0.norm2.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.1.conv1.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.1.conv1.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.2.block.1.conv2.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.1.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.down.2.block.1.norm1.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.1.norm1.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.1.norm2.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.down.2.block.1.norm2.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.attn_1.k.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.attn_1.k.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.mid.attn_1.norm.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.attn_1.norm.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.attn_1.proj_out.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.attn_1.proj_out.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.mid.attn_1.q.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.attn_1.q.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.mid.attn_1.v.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.attn_1.v.weight: torch.Size([256, 256, 1, 1, 1])
first_stage_model.encoder.cnn_encoder.mid.block_1.conv1.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_1.conv1.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.mid.block_1.conv2.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_1.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.mid.block_1.norm1.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_1.norm1.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_1.norm2.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_1.norm2.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_2.conv1.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_2.conv1.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.mid.block_2.conv2.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_2.conv2.weight: torch.Size([256, 256, 3, 3, 3])
first_stage_model.encoder.cnn_encoder.mid.block_2.norm1.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_2.norm1.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_2.norm2.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.mid.block_2.norm2.weight: torch.Size([256])
first_stage_model.encoder.cnn_encoder.norm_out.bias: torch.Size([256])
first_stage_model.encoder.cnn_encoder.norm_out.weight: torch.Size([256])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.0.bias: torch.Size([64])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.0.weight: torch.Size([64, 32])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.1.bias: torch.Size([64])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.1.weight: torch.Size([64, 64])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.2.bias: torch.Size([64])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.2.weight: torch.Size([64, 64])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.3.bias: torch.Size([64])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.3.weight: torch.Size([64, 64])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.4.bias: torch.Size([64])
first_stage_model.encoder.gino_encoder.gno_in.mlp.fcs.4.weight: torch.Size([64, 64])
first_stage_model.encoder.gino_encoder.pos_embed.linear.bias: torch.Size([8])
first_stage_model.encoder.gino_encoder.pos_embed.linear.weight: torch.Size([8, 3])
first_stage_model.encoder.gino_encoder.x_projection.fcs.0.bias: torch.Size([64])
first_stage_model.encoder.gino_encoder.x_projection.fcs.0.weight: torch.Size([64, 3])
first_stage_model.latent_grid: torch.Size([1, 64, 64, 64, 3])
log_one_minus_alphas_cumprod: torch.Size([1000])
loss_weight: torch.Size([1000])
model.diffusion_model.blocks.0.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.0.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.0.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.0.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.0.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.0.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.0.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.0.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.0.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.0.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.0.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.0.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.0.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.0.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.0.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.1.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.1.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.1.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.1.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.1.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.1.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.1.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.1.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.1.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.1.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.1.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.1.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.1.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.1.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.1.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.10.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.10.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.10.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.10.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.10.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.10.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.10.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.10.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.10.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.10.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.10.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.10.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.10.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.10.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.10.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.11.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.11.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.11.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.11.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.11.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.11.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.11.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.11.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.11.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.11.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.11.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.11.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.11.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.11.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.11.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.12.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.12.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.12.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.12.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.12.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.12.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.12.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.12.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.12.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.12.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.12.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.12.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.12.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.12.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.12.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.13.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.13.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.13.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.13.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.13.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.13.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.13.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.13.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.13.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.13.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.13.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.13.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.13.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.13.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.13.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.14.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.14.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.14.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.14.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.14.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.14.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.14.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.14.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.14.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.14.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.14.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.14.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.14.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.14.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.14.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.15.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.15.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.15.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.15.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.15.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.15.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.15.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.15.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.15.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.15.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.15.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.15.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.15.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.15.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.15.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.16.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.16.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.16.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.16.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.16.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.16.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.16.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.16.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.16.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.16.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.16.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.16.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.16.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.16.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.16.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.17.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.17.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.17.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.17.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.17.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.17.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.17.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.17.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.17.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.17.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.17.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.17.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.17.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.17.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.17.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.18.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.18.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.18.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.18.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.18.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.18.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.18.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.18.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.18.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.18.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.18.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.18.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.18.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.18.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.18.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.19.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.19.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.19.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.19.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.19.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.19.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.19.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.19.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.19.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.19.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.19.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.19.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.19.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.19.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.19.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.2.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.2.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.2.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.2.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.2.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.2.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.2.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.2.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.2.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.2.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.2.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.2.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.2.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.2.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.2.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.20.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.20.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.20.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.20.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.20.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.20.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.20.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.20.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.20.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.20.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.20.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.20.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.20.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.20.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.20.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.21.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.21.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.21.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.21.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.21.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.21.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.21.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.21.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.21.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.21.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.21.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.21.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.21.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.21.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.21.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.22.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.22.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.22.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.22.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.22.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.22.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.22.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.22.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.22.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.22.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.22.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.22.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.22.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.22.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.22.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.23.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.23.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.23.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.23.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.23.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.23.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.23.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.23.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.23.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.23.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.23.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.23.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.23.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.23.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.23.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.24.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.24.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.24.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.24.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.24.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.24.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.24.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.24.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.24.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.24.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.24.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.24.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.24.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.24.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.24.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.25.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.25.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.25.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.25.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.25.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.25.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.25.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.25.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.25.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.25.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.25.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.25.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.25.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.25.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.25.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.26.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.26.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.26.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.26.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.26.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.26.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.26.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.26.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.26.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.26.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.26.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.26.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.26.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.26.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.26.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.27.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.27.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.27.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.27.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.27.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.27.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.27.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.27.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.27.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.27.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.27.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.27.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.27.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.27.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.27.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.3.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.3.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.3.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.3.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.3.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.3.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.3.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.3.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.3.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.3.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.3.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.3.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.3.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.3.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.3.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.4.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.4.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.4.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.4.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.4.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.4.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.4.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.4.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.4.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.4.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.4.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.4.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.4.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.4.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.4.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.5.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.5.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.5.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.5.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.5.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.5.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.5.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.5.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.5.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.5.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.5.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.5.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.5.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.5.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.5.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.6.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.6.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.6.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.6.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.6.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.6.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.6.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.6.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.6.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.6.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.6.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.6.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.6.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.6.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.6.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.7.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.7.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.7.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.7.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.7.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.7.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.7.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.7.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.7.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.7.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.7.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.7.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.7.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.7.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.7.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.8.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.8.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.8.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.8.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.8.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.8.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.8.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.8.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.8.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.8.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.8.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.8.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.8.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.8.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.8.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.blocks.9.adaLN_modulation.1.bias: torch.Size([6144])
model.diffusion_model.blocks.9.adaLN_modulation.1.weight: torch.Size([6144, 1024])
model.diffusion_model.blocks.9.attn.proj.bias: torch.Size([1024])
model.diffusion_model.blocks.9.attn.proj.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.9.attn.qkv.bias: torch.Size([3072])
model.diffusion_model.blocks.9.attn.qkv.weight: torch.Size([3072, 1024])
model.diffusion_model.blocks.9.cross_attn.to_k.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.9.cross_attn.to_out.0.bias: torch.Size([1024])
model.diffusion_model.blocks.9.cross_attn.to_out.0.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.9.cross_attn.to_q.weight: torch.Size([1024, 1024])
model.diffusion_model.blocks.9.cross_attn.to_v.weight: torch.Size([1024, 768])
model.diffusion_model.blocks.9.mlp.fc1.bias: torch.Size([4096])
model.diffusion_model.blocks.9.mlp.fc1.weight: torch.Size([4096, 1024])
model.diffusion_model.blocks.9.mlp.fc2.bias: torch.Size([1024])
model.diffusion_model.blocks.9.mlp.fc2.weight: torch.Size([1024, 4096])
model.diffusion_model.final_layer.adaLN_modulation.1.bias: torch.Size([2048])
model.diffusion_model.final_layer.adaLN_modulation.1.weight: torch.Size([2048, 1024])
model.diffusion_model.final_layer.linear.bias: torch.Size([128])
model.diffusion_model.final_layer.linear.weight: torch.Size([128, 1024])
model.diffusion_model.pos_embed: torch.Size([1, 512, 1024])
model.diffusion_model.t_embedder.mlp.0.bias: torch.Size([1024])
model.diffusion_model.t_embedder.mlp.0.weight: torch.Size([1024, 256])
model.diffusion_model.t_embedder.mlp.2.bias: torch.Size([1024])
model.diffusion_model.t_embedder.mlp.2.weight: torch.Size([1024, 1024])
model.diffusion_model.x_embedder.proj.bias: torch.Size([1024])
model.diffusion_model.x_embedder.proj.weight: torch.Size([1024, 16, 2, 2, 2])
model.diffusion_model.y_embedder.0.bias: torch.Size([1024])
model.diffusion_model.y_embedder.0.weight: torch.Size([1024, 768])
model.diffusion_model.y_embedder.2.bias: torch.Size([1024])
model.diffusion_model.y_embedder.2.weight: torch.Size([1024, 1024])
posterior_log_variance_clipped: torch.Size([1000])
posterior_mean_coef1: torch.Size([1000])
posterior_mean_coef2: torch.Size([1000])
posterior_variance: torch.Size([1000])
scale_factor: torch.Size([])
sqrt_alphas_cumprod: torch.Size([1000])
sqrt_one_minus_alphas_cumprod: torch.Size([1000])
sqrt_recip_alphas_cumprod: torch.Size([1000])
sqrt_recipm1_alphas_cumprod: torch.Size([1000])
